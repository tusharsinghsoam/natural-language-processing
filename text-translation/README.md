# About

## Neural Machine Translation with Attention

1. This project demonstrates how to train a sequence-to-sequence (seq2seq) model for Spanish-to-English translation roughly based on [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025v5) (Luong et al., 2015).

2. While this architecture is somewhat outdated, it is still a very useful project to work through to get a deeper understanding of sequence-to-sequence models and attention mechanisms (before going on to [Transformers](transformer.ipynb)).

3. After training the model in this notebook, you will be able to input a Spanish sentence, such as "*Â¿todavia estan en casa?*", and return the English translation: "*are you still at home?*"

## Neural Machine Translation with a Transformer

1. This project demonstrates how to create and train a [sequence-to-sequence](https://developers.google.com/machine-learning/glossary#sequence-to-sequence-task) [Transformer](https://developers.google.com/machine-learning/glossary#Transformer) model to translate [Portuguese into English](https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate#ted_hrlr_translatept_to_en). The Transformer was originally proposed in ["Attention is all you need"](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017).

2. Transformers are deep neural networks that replace CNNs and RNNs with [self-attention](https://developers.google.com/machine-learning/glossary#self-attention). Self-attention allows Transformers to easily transmit information across the input sequences.

3. A Transformer is a sequence-to-sequence encoder-decoder model similar to the model in the NMT with attention tutorial. A single-layer Transformer takes a little more code to write, but is almost identical to that encoder-decoder RNN model. The only difference is that the RNN layers are replaced with self-attention layers. This project builds a 4-layer Transformer which is larger and more powerful, but not fundamentally more complex.


## Acknowledgement
1. [Neural machine translation with attention](https://www.tensorflow.org/text/tutorials/nmt_with_attention) tutorial.
2. [Neural machine translation with a Transformer](https://www.tensorflow.org/text/tutorials/transformer) tutorial.