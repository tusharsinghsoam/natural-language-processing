{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fcb5d7e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "468607b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 21:25:27.613869: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "787771d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.57.3\n",
      "Pytorch version: 2.2.2\n",
      "Dataset version: 4.0.0\n",
      "Evaluate version:  0.4.6\n",
      "Python 3.11.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Pytorch version:\", torch.__version__)\n",
    "print(\"Dataset version:\", datasets.__version__)\n",
    "print(\"Evaluate version: \", evaluate.__version__)\n",
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cb171b",
   "metadata": {},
   "source": [
    "# Loading the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f20dce5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import DistilBertForQuestionAnswering\n",
    "\n",
    "trained_checkpoint = \"distilbert/distilbert-base-uncased\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(trained_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a531b210",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce17c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"squad\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e5868",
   "metadata": {},
   "source": [
    "## Visualize some examples from the SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dedeb22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTrain Data Sample.....\u001b[0;0m\n",
      " \n",
      "\u001b[1mID -\u001b[0;0m 5733be284776f41900661182\n",
      "\u001b[1mTITLE - \u001b[0;0m University_of_Notre_Dame\n",
      "\u001b[1mCONTEXT - \u001b[0;0m Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "\u001b[1mQUESTION - \u001b[0;0m To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "\u001b[1mANSWERS - \u001b[0;0m ['Saint Bernadette Soubirous']\n",
      "\u001b[1mANSWERS START INDEX - \u001b[0;0m [515]\n",
      " \n",
      "------------------------------------------------------------------------------------------\n",
      "\u001b[1mValidation Data Sample.....\u001b[0;0m\n",
      " \n",
      "\u001b[1mID -\u001b[0;0m 56be4db0acb8001400a502ec\n",
      "\u001b[1mTITLE - \u001b[0;0m Super_Bowl_50\n",
      "\u001b[1mCONTEXT - \u001b[0;0m Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\u001b[1mQUESTION - \u001b[0;0m Which NFL team represented the AFC at Super Bowl 50?\n",
      "\u001b[1mANSWERS - \u001b[0;0m ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\n",
      "\u001b[1mANSWERS START INDEX - \u001b[0;0m [177, 177, 177]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# to make text bold\n",
    "s_bold = '\\033[1m'\n",
    "e_bold = '\\033[0;0m'\n",
    "\n",
    "print(s_bold + 'Train Data Sample.....' + e_bold)\n",
    "train_data = dataset[\"train\"]\n",
    "for data in train_data:\n",
    "    print(' ')\n",
    "    print(s_bold + 'ID -' + e_bold, data['id'])\n",
    "    print(s_bold +'TITLE - '+ e_bold, data['title'])\n",
    "    print(s_bold + 'CONTEXT - '+ e_bold,data['context'])\n",
    "    print(s_bold + 'QUESTION - '+ e_bold,data['question'])\n",
    "    print(s_bold + 'ANSWERS - ' + e_bold,data['answers']['text'])\n",
    "    print(s_bold + 'ANSWERS START INDEX - ' + e_bold,data['answers']['answer_start'])\n",
    "    print(' ')\n",
    "    break\n",
    "    \n",
    "print('---'*30)   \n",
    "print(s_bold + 'Validation Data Sample.....' + e_bold)\n",
    "train_data = dataset[\"validation\"]\n",
    "for data in train_data:\n",
    "    print(' ')\n",
    "    print(s_bold + 'ID -' + e_bold, data['id'])\n",
    "    print(s_bold +'TITLE - '+ e_bold, data['title'])\n",
    "    print(s_bold + 'CONTEXT - '+ e_bold,data['context'])\n",
    "    print(s_bold + 'QUESTION - '+ e_bold,data['question'])\n",
    "    print(s_bold + 'ANSWERS - ' + e_bold,data['answers']['text'])\n",
    "    print(s_bold + 'ANSWERS START INDEX - ' + e_bold,data['answers']['answer_start'])\n",
    "    print(' ')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810fb12e",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "927528e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89cae63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 10567\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "161c0eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lets sample some dataset so that we can reduce training time.\n",
    "dataset[\"train\"] = dataset[\"train\"].select([i for i in range(10000)])\n",
    "dataset[\"validation\"] = dataset[\"validation\"].select([i for i in range(2000)])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54eda0e",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffb7021",
   "metadata": {},
   "source": [
    "### Handling long contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f07fab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This example was split into 2 chunks/features.\n",
      "Here is where each comes from: [0, 0].\n",
      "Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      " \n",
      "Context :  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      " \n",
      "Answer:  ['Saint Bernadette Soubirous']\n",
      "--------------------------------------------------\n",
      "Context piece 1\n",
      "Context piece 1 has length 160 tokens.\n",
      "[SEP] architecturally, the school has a catholic character. atop the main building ' s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues [SEP]\n",
      " \n",
      "Context piece 2\n",
      "Context piece 2 has length 104 tokens.\n",
      "[SEP] sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "context = dataset[\"train\"][0][\"context\"]\n",
    "question = dataset[\"train\"][0][\"question\"]\n",
    "answer = dataset[\"train\"][0][\"answers\"][\"text\"]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=160,\n",
    "    truncation=\"only_second\",  # only to truncate context\n",
    "    stride=70,  # no of overlapping tokens  between concecute context pieces\n",
    "    return_overflowing_tokens=True,  #to let tokenizer know we want overflow tokens\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "print(f\"This example was split into {len(inputs['input_ids'])} chunks/features.\")\n",
    "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")\n",
    "\n",
    "print('Question: ',question)\n",
    "print(' ')\n",
    "print('Context : ',context)\n",
    "print(' ')\n",
    "print('Answer: ', answer)\n",
    "print('--'*25)\n",
    "\n",
    "for i, ids in enumerate(inputs[\"input_ids\"]):\n",
    "    print('Context piece', i+1)\n",
    "    print(f'Context piece {i+1} has length {len(ids)} tokens.')\n",
    "    print(tokenizer.decode(ids[ids.index(102):]))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdec99a",
   "metadata": {},
   "source": [
    "### Understanding Tokenizer Attributes\n",
    "\n",
    "Let's explore the key attributes returned by the tokenizer that are crucial for QA tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efbeec7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXAMINING CHUNK 0\n",
      "================================================================================\n",
      "\n",
      "1. INPUT IDS:\n",
      "   Token IDs: [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 6549, 2135, 1010]...\n",
      "   Total tokens: 160\n",
      "   Decoded text: [CLS] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP] architecturally, the school has a catholic character. atop the mai...\n",
      "\n",
      "2. OFFSET MAPPING (Character positions):\n",
      "   Maps each token to its character position in the COMBINED question+context text\n",
      "   First 10 token offsets: [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47)]\n",
      "\n",
      "   Detailed Token -> Character -> Original Text mapping:\n",
      "   (Showing tokens 0-15)\n",
      "    Pos |      Token      |  Char Span   | Original Text Segment\n",
      "   -----+-----------------+--------------+-----------------------------------------\n",
      "      0 |      [CLS]      |    (0,0)     | ''\n",
      "      1 |       to        |    (0,2)     | 'To'\n",
      "      2 |      whom       |    (3,7)     | 'whom'\n",
      "      3 |       did       |    (8,11)    | 'did'\n",
      "      4 |       the       |   (12,15)    | 'the'\n",
      "      5 |     virgin      |   (16,22)    | 'Virgin'\n",
      "      6 |      mary       |   (23,27)    | 'Mary'\n",
      "      7 |    allegedly    |   (28,37)    | 'allegedly'\n",
      "      8 |     appear      |   (38,44)    | 'appear'\n",
      "      9 |       in        |   (45,47)    | 'in'\n",
      "     10 |      1858       |   (48,52)    | '1858'\n",
      "     11 |       in        |   (53,55)    | 'in'\n",
      "     12 |       lou       |   (56,59)    | 'Lou'\n",
      "     13 |     ##rdes      |   (59,63)    | 'rdes'\n",
      "     14 |     france      |   (64,70)    | 'France'\n",
      "     15 |        ?        |   (70,71)    | '?'\n",
      "     16 |      [SEP]      |    (0,0)     | ''\n",
      "     17 |  architectural  |    (0,13)    | 'Architectural'\n",
      "     18 |      ##ly       |   (13,15)    | 'ly'\n",
      "     19 |        ,        |   (15,16)    | ','\n",
      "     20 |       the       |   (17,20)    | 'the'\n",
      "     21 |     school      |   (21,27)    | 'school'\n",
      "     22 |       has       |   (28,31)    | 'has'\n",
      "     23 |        a        |   (32,33)    | 'a'\n",
      "     24 |    catholic     |   (34,42)    | 'Catholic'\n",
      "     25 |    character    |   (43,52)    | 'character'\n",
      "     26 |        .        |   (52,53)    | '.'\n",
      "     27 |      atop       |   (54,58)    | 'Atop'\n",
      "     28 |       the       |   (59,62)    | 'the'\n",
      "     29 |      main       |   (63,67)    | 'Main'\n",
      "\n",
      "3. OVERFLOW_TO_SAMPLE_MAPPING:\n",
      "   Shows which original example each chunk came from\n",
      "   All chunks: [0, 0]\n",
      "   Chunk 0 came from original example: 0\n",
      "\n",
      "4. SEQUENCE_IDS (Question=0, Context=1, Special=None):\n",
      "   Sequence IDs: [None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n",
      "   Question tokens (0): 15\n",
      "   Context tokens (1): 142\n",
      "   Special tokens (None): 3\n",
      "\n",
      "   Visual breakdown with original text:\n",
      "    Pos |   Type   |      Token      |  Char Span  \n",
      "   -----+----------+-----------------+--------------+-------------------------------\n",
      "      0 | SPECIAL  |      [CLS]      |    (0,0)    '\n",
      "      1 | QUESTION |       to        |    (0,2)    '\n",
      "      2 | QUESTION |      whom       |    (3,7)    '\n",
      "      3 | QUESTION |       did       |    (8,11)   '\n",
      "      4 | QUESTION |       the       |   (12,15)   '\n",
      "      5 | QUESTION |     virgin      |   (16,22)   '\n",
      "      6 | QUESTION |      mary       |   (23,27)   '\n",
      "      7 | QUESTION |    allegedly    |   (28,37)   '\n",
      "      8 | QUESTION |     appear      |   (38,44)   '\n",
      "      9 | QUESTION |       in        |   (45,47)   '\n",
      "     10 | QUESTION |      1858       |   (48,52)   '\n",
      "     11 | QUESTION |       in        |   (53,55)   '\n",
      "     12 | QUESTION |       lou       |   (56,59)   '\n",
      "     13 | QUESTION |     ##rdes      |   (59,63)   '\n",
      "     14 | QUESTION |     france      |   (64,70)   '\n",
      "     15 | QUESTION |        ?        |   (70,71)   '\n",
      "     16 | SPECIAL  |      [SEP]      |    (0,0)    '\n",
      "     17 | CONTEXT  |  architectural  |    (0,13)   '\n",
      "     18 | CONTEXT  |      ##ly       |   (13,15)   '\n",
      "     19 | CONTEXT  |        ,        |   (15,16)   '\n",
      "     20 | CONTEXT  |       the       |   (17,20)   '\n",
      "     21 | CONTEXT  |     school      |   (21,27)   '\n",
      "     22 | CONTEXT  |       has       |   (28,31)   '\n",
      "     23 | CONTEXT  |        a        |   (32,33)   '\n",
      "     24 | CONTEXT  |    catholic     |   (34,42)   '\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Let's examine one chunk/feature in detail\n",
    "chunk_idx = 0\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"EXAMINING CHUNK {chunk_idx}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Input IDs - the tokenized representation\n",
    "print(\"\\n1. INPUT IDS:\")\n",
    "print(f\"   Token IDs: {inputs['input_ids'][chunk_idx][:20]}...\")  # showing first 20\n",
    "print(f\"   Total tokens: {len(inputs['input_ids'][chunk_idx])}\")\n",
    "\n",
    "# Decode to see the actual text\n",
    "decoded_text = tokenizer.decode(inputs['input_ids'][chunk_idx])\n",
    "print(f\"   Decoded text: {decoded_text[:150]}...\")\n",
    "\n",
    "# 2. Offset Mapping - character positions in original text\n",
    "print(\"\\n2. OFFSET MAPPING (Character positions):\")\n",
    "print(f\"   Maps each token to its character position in the COMBINED question+context text\")\n",
    "print(f\"   First 10 token offsets: {inputs['offset_mapping'][chunk_idx][:10]}\")\n",
    "\n",
    "print(f\"\\n   Detailed Token -> Character -> Original Text mapping:\")\n",
    "print(f\"   (Showing tokens 0-15)\")\n",
    "print(f\"   {'Pos':>4} | {'Token':^15} | {'Char Span':^12} | Original Text Segment\")\n",
    "print(f\"   {'-'*4}-+-{'-'*15}-+-{'-'*12}-+-{'-'*40}\")\n",
    "\n",
    "for i in range(min(30, len(inputs['input_ids'][chunk_idx]))):\n",
    "    token_id = inputs['input_ids'][chunk_idx][i]\n",
    "    token = tokenizer.decode([token_id])\n",
    "    char_span = inputs['offset_mapping'][chunk_idx][i]\n",
    "    seq_id = inputs.sequence_ids(chunk_idx)[i]\n",
    "\n",
    "    if seq_id == 0 or seq_id is None:\n",
    "        original_text_segment = question[char_span[0]:char_span[1]]\n",
    "    elif seq_id == 1:\n",
    "        original_text_segment = context[char_span[0]:char_span[1]]\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    span_str = f\"({char_span[0]},{char_span[1]})\"\n",
    "    print(f\"   {i:>4} | {token:^15} | {span_str:^12} | '{original_text_segment}'\")\n",
    "\n",
    "# 3. Overflow to Sample Mapping\n",
    "print(\"\\n3. OVERFLOW_TO_SAMPLE_MAPPING:\")\n",
    "print(f\"   Shows which original example each chunk came from\")\n",
    "print(f\"   All chunks: {inputs['overflow_to_sample_mapping']}\")\n",
    "print(f\"   Chunk {chunk_idx} came from original example: {inputs['overflow_to_sample_mapping'][chunk_idx]}\")\n",
    "\n",
    "# 4. Sequence IDs - which part is question vs context\n",
    "print(\"\\n4. SEQUENCE_IDS (Question=0, Context=1, Special=None):\")\n",
    "sequence_ids = inputs.sequence_ids(chunk_idx)\n",
    "print(f\"   Sequence IDs: {sequence_ids[:30]}...\")\n",
    "\n",
    "# Find boundaries\n",
    "question_tokens = sum(1 for sid in sequence_ids if sid == 0)\n",
    "context_tokens = sum(1 for sid in sequence_ids if sid == 1)\n",
    "special_tokens = sum(1 for sid in sequence_ids if sid is None)\n",
    "\n",
    "print(f\"   Question tokens (0): {question_tokens}\")\n",
    "print(f\"   Context tokens (1): {context_tokens}\")\n",
    "print(f\"   Special tokens (None): {special_tokens}\")\n",
    "\n",
    "# Show the breakdown visually with original text\n",
    "print(\"\\n   Visual breakdown with original text:\")\n",
    "print(f\"   {'Pos':>4} | {'Type':^8} | {'Token':^15} | {'Char Span':^12}\")\n",
    "print(f\"   {'-'*4}-+-{'-'*8}-+-{'-'*15}-+-{'-'*12}-+-{'-'*30}\")\n",
    "\n",
    "for i in range(min(25, len(sequence_ids))):\n",
    "    token = tokenizer.decode([inputs['input_ids'][chunk_idx][i]])\n",
    "    seq_id = sequence_ids[i]\n",
    "    char_span = inputs['offset_mapping'][chunk_idx][i]\n",
    "    \n",
    "    if seq_id is None:\n",
    "        label = \"SPECIAL\"\n",
    "    elif seq_id == 0:\n",
    "        label = \"QUESTION\"\n",
    "    else:\n",
    "        label = \"CONTEXT\"\n",
    "    \n",
    "    span_str = f\"({char_span[0]},{char_span[1]})\"\n",
    "    print(f\"   {i:>4} | {label:^8} | {token:^15} | {span_str:^12}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "693c310f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNDERSTANDING STRIDE AND OVERLAPPING CHUNKS\n",
      "================================================================================\n",
      "max_length: 160\n",
      "stride: 70 tokens\n",
      "Number of chunks created: 2\n",
      "\n",
      "CHUNK 1 - Context has 142 tokens\n",
      "CHUNK 2 - Context has 86 tokens\n",
      "\n",
      "ACTUAL OVERLAPPING TOKENS: 70 tokens\n",
      "\n",
      "Showing 20 of 70 overlapping tokens:\n",
      " Chunk1 Pos  |  Chunk2 Pos  |  Token ID  |   Char Pos   | Token Text\n",
      "--------------------------------------------------------------------------------\n",
      "     89      |      17      |    6730    |   320-326    | 'sacred'\n",
      "     90      |      18      |    2540    |   327-332    | 'heart'\n",
      "     91      |      19      |    1012    |   332-333    | '.'\n",
      "     92      |      20      |    3202    |   334-345    | 'immediately'\n",
      "     93      |      21      |    2369    |   346-352    | 'behind'\n",
      "     94      |      22      |    1996    |   353-356    | 'the'\n",
      "     95      |      23      |   13546    |   357-365    | 'basilica'\n",
      "     96      |      24      |    2003    |   366-368    | 'is'\n",
      "     97      |      25      |    1996    |   369-372    | 'the'\n",
      "     98      |      26      |   24665    |   373-375    | 'gr'\n",
      "     99      |      27      |   23052    |   375-379    | '##otto'\n",
      "    100      |      28      |    1010    |   379-380    | ','\n",
      "    101      |      29      |    1037    |   381-382    | 'a'\n",
      "    102      |      30      |   14042    |   383-389    | 'marian'\n",
      "    103      |      31      |    2173    |   390-395    | 'place'\n",
      "    104      |      32      |    1997    |   396-398    | 'of'\n",
      "    105      |      33      |    7083    |   399-405    | 'prayer'\n",
      "    106      |      34      |    1998    |   406-409    | 'and'\n",
      "    107      |      35      |    9185    |   410-420    | 'reflection'\n",
      "    108      |      36      |    1012    |   420-421    | '.'\n",
      "... and 50 more overlapping tokens\n",
      "\n",
      "================================================================================\n",
      "DECODED OVERLAPPING TEXT:\n",
      "sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues\n",
      "\n",
      "ðŸ‘† These tokens appear in BOTH chunks at the same character positions!\n",
      "   This sliding window approach ensures answers aren't split across chunks.\n",
      "\n",
      "\n",
      "WHY OFFSET_MAPPING MATTERS FOR QA:\n",
      "--------------------------------------------------------------------------------\n",
      "Original answer: ['Saint Bernadette Soubirous']\n",
      "Answer starts at character: 515\n",
      "Answer ends at character: 541\n",
      "Answer span: [515, 541)\n",
      "\n",
      "Using offset_mapping, we can find which TOKENS contain the answer:\n",
      "[{'idx': 130, 'token': 'saint', 'span': (515, 520), 'context_text': 'Saint'}, {'idx': 131, 'token': 'bern', 'span': (521, 525), 'context_text': 'Bern'}, {'idx': 132, 'token': '##ade', 'span': (525, 528), 'context_text': 'ade'}, {'idx': 133, 'token': '##tte', 'span': (528, 531), 'context_text': 'tte'}, {'idx': 134, 'token': 'so', 'span': (532, 534), 'context_text': 'So'}, {'idx': 135, 'token': '##ub', 'span': (534, 536), 'context_text': 'ub'}, {'idx': 136, 'token': '##iro', 'span': (536, 539), 'context_text': 'iro'}, {'idx': 137, 'token': '##us', 'span': (539, 541), 'context_text': 'us'}]\n",
      "\n",
      "  Chunk 0 - Found 8 answer tokens:\n",
      "    Token 130: 'saint' (chars 515-520) -> Context: 'Saint'\n",
      "    Token 131: 'bern' (chars 521-525) -> Context: 'Bern'\n",
      "    Token 132: '##ade' (chars 525-528) -> Context: 'ade'\n",
      "    Token 133: '##tte' (chars 528-531) -> Context: 'tte'\n",
      "    Token 134: 'so' (chars 532-534) -> Context: 'So'\n",
      "    Token 135: '##ub' (chars 534-536) -> Context: 'ub'\n",
      "    Token 136: '##iro' (chars 536-539) -> Context: 'iro'\n",
      "    Token 137: '##us' (chars 539-541) -> Context: 'us'\n",
      "    Reconstructed answer: 'saint bernadette soubirous'\n",
      "    Original answer:      'Saint Bernadette Soubirous'\n",
      "    Match: True\n",
      "[{'idx': 58, 'token': 'saint', 'span': (515, 520), 'context_text': 'Saint'}, {'idx': 59, 'token': 'bern', 'span': (521, 525), 'context_text': 'Bern'}, {'idx': 60, 'token': '##ade', 'span': (525, 528), 'context_text': 'ade'}, {'idx': 61, 'token': '##tte', 'span': (528, 531), 'context_text': 'tte'}, {'idx': 62, 'token': 'so', 'span': (532, 534), 'context_text': 'So'}, {'idx': 63, 'token': '##ub', 'span': (534, 536), 'context_text': 'ub'}, {'idx': 64, 'token': '##iro', 'span': (536, 539), 'context_text': 'iro'}, {'idx': 65, 'token': '##us', 'span': (539, 541), 'context_text': 'us'}]\n",
      "\n",
      "  Chunk 1 - Found 8 answer tokens:\n",
      "    Token 58: 'saint' (chars 515-520) -> Context: 'Saint'\n",
      "    Token 59: 'bern' (chars 521-525) -> Context: 'Bern'\n",
      "    Token 60: '##ade' (chars 525-528) -> Context: 'ade'\n",
      "    Token 61: '##tte' (chars 528-531) -> Context: 'tte'\n",
      "    Token 62: 'so' (chars 532-534) -> Context: 'So'\n",
      "    Token 63: '##ub' (chars 534-536) -> Context: 'ub'\n",
      "    Token 64: '##iro' (chars 536-539) -> Context: 'iro'\n",
      "    Token 65: '##us' (chars 539-541) -> Context: 'us'\n",
      "    Reconstructed answer: 'saint bernadette soubirous'\n",
      "    Original answer:      'Saint Bernadette Soubirous'\n",
      "    Match: True\n",
      "\n",
      "================================================================================\n",
      "\n",
      "KEY INSIGHT:\n",
      "The condition 'start_char < answer_end_char AND end_char > answer_start_char'\n",
      "ensures we capture ALL tokens overlapping with the answer, not just the first\n",
      "and last tokens. This is crucial for multi-token answers!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Visualizing how stride and max_length create overlapping chunks\n",
    "print(\"UNDERSTANDING STRIDE AND OVERLAPPING CHUNKS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"max_length: {160}\")\n",
    "print(f\"stride: {70} tokens\")\n",
    "print(f\"Number of chunks created: {len(inputs['input_ids'])}\\n\")\n",
    "\n",
    "# Show the overlap between consecutive chunks using offset_mapping\n",
    "if len(inputs['input_ids']) > 1:\n",
    "    chunk1_tokens = inputs['input_ids'][0]\n",
    "    chunk2_tokens = inputs['input_ids'][1]\n",
    "    \n",
    "    # Get offset mappings and sequence IDs for both chunks\n",
    "    chunk1_offsets = inputs['offset_mapping'][0]\n",
    "    chunk2_offsets = inputs['offset_mapping'][1]\n",
    "    chunk1_seq_ids = inputs.sequence_ids(0)\n",
    "    chunk2_seq_ids = inputs.sequence_ids(1)\n",
    "    \n",
    "    # Extract context tokens (sequence_id == 1) with their offsets\n",
    "    chunk1_context = []\n",
    "    for idx, (token_id, offset, seq_id) in enumerate(zip(chunk1_tokens, chunk1_offsets, chunk1_seq_ids)):\n",
    "        if seq_id == 1 and token_id != 0:  # Context tokens, not padding\n",
    "            chunk1_context.append({\n",
    "                'token_idx': idx,\n",
    "                'token_id': token_id,\n",
    "                'char_start': offset[0],\n",
    "                'char_end': offset[1],\n",
    "                'token_text': tokenizer.decode([token_id])\n",
    "            })\n",
    "    \n",
    "    chunk2_context = []\n",
    "    for idx, (token_id, offset, seq_id) in enumerate(zip(chunk2_tokens, chunk2_offsets, chunk2_seq_ids)):\n",
    "        if seq_id == 1 and token_id != 0:  # Context tokens, not padding\n",
    "            chunk2_context.append({\n",
    "                'token_idx': idx,\n",
    "                'token_id': token_id,\n",
    "                'char_start': offset[0],\n",
    "                'char_end': offset[1],\n",
    "                'token_text': tokenizer.decode([token_id])\n",
    "            })\n",
    "    \n",
    "    print(f\"CHUNK 1 - Context has {len(chunk1_context)} tokens\")\n",
    "    print(f\"CHUNK 2 - Context has {len(chunk2_context)} tokens\\n\")\n",
    "    \n",
    "    # Find overlapping tokens by comparing character offsets\n",
    "    # Tokens overlap if they reference the same character positions in the original text\n",
    "    overlapping_tokens = []\n",
    "    for token1 in chunk1_context:\n",
    "        for token2 in chunk2_context:\n",
    "            # Check if tokens overlap based on character positions\n",
    "            if (token1['char_start'] == token2['char_start'] and \n",
    "                token1['char_end'] == token2['char_end']):\n",
    "                overlapping_tokens.append({\n",
    "                    'chunk1_idx': token1['token_idx'],\n",
    "                    'chunk2_idx': token2['token_idx'],\n",
    "                    'token_id': token1['token_id'],\n",
    "                    'token_text': token1['token_text'],\n",
    "                    'char_pos': f\"{token1['char_start']}-{token1['char_end']}\"\n",
    "                })\n",
    "                break\n",
    "    \n",
    "    print(f\"ACTUAL OVERLAPPING TOKENS: {len(overlapping_tokens)} tokens\\n\")\n",
    "    \n",
    "    if overlapping_tokens:\n",
    "        # Display overlapping tokens\n",
    "        display_count = min(20, len(overlapping_tokens))\n",
    "        \n",
    "        print(f\"Showing {display_count} of {len(overlapping_tokens)} overlapping tokens:\")\n",
    "        print(f\"{'Chunk1 Pos':^12} | {'Chunk2 Pos':^12} | {'Token ID':^10} | {'Char Pos':^12} | Token Text\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, token_info in enumerate(overlapping_tokens[:display_count]):\n",
    "            print(f\"{token_info['chunk1_idx']:^12} | {token_info['chunk2_idx']:^12} | \"\n",
    "                  f\"{token_info['token_id']:^10} | {token_info['char_pos']:^12} | '{token_info['token_text']}'\")\n",
    "        \n",
    "        if len(overlapping_tokens) > display_count:\n",
    "            print(f\"... and {len(overlapping_tokens) - display_count} more overlapping tokens\")\n",
    "        \n",
    "        # Show the decoded text of overlapping portion\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DECODED OVERLAPPING TEXT:\")\n",
    "        overlap_token_ids = [t['token_id'] for t in overlapping_tokens]\n",
    "        overlap_text = tokenizer.decode(overlap_token_ids)\n",
    "        print(f\"{overlap_text}\")\n",
    "        \n",
    "        print(\"\\nðŸ‘† These tokens appear in BOTH chunks at the same character positions!\")\n",
    "        print(\"   This sliding window approach ensures answers aren't split across chunks.\\n\")\n",
    "    else:\n",
    "        print(\"No overlapping tokens found. This might happen if context is very short.\\n\")\n",
    "\n",
    "# Demonstrate why offset_mapping is crucial for finding answers\n",
    "print(\"\\nWHY OFFSET_MAPPING MATTERS FOR QA:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Original answer:\", answer)\n",
    "print(f\"Answer starts at character: {dataset['train'][0]['answers']['answer_start'][0]}\")\n",
    "\n",
    "# Find which tokens contain the answer using offset_mapping\n",
    "answer_start_char = dataset['train'][0]['answers']['answer_start'][0]\n",
    "answer_end_char = answer_start_char + len(answer[0])\n",
    "\n",
    "print(f\"Answer ends at character: {answer_end_char}\")\n",
    "print(f\"Answer span: [{answer_start_char}, {answer_end_char})\")\n",
    "print(f\"\\nUsing offset_mapping, we can find which TOKENS contain the answer:\")\n",
    "\n",
    "for chunk_idx in range(len(inputs['input_ids'])):\n",
    "    offsets = inputs['offset_mapping'][chunk_idx]\n",
    "    sequence_ids = inputs.sequence_ids(chunk_idx)\n",
    "    \n",
    "    found_answer_tokens = []\n",
    "    # Check if answer is in this chunk\n",
    "    for token_idx, (start_char, end_char) in enumerate(offsets):\n",
    "        # Skip question tokens and special tokens\n",
    "        if sequence_ids[token_idx] != 1:\n",
    "            continue\n",
    "        \n",
    "        # CORRECT condition: token overlaps with answer if:\n",
    "        # token_start < answer_end AND token_end > answer_start\n",
    "        # This captures ALL tokens that have any overlap with the answer span\n",
    "        if start_char < answer_end_char and end_char > answer_start_char:\n",
    "            token = tokenizer.decode([inputs['input_ids'][chunk_idx][token_idx]])\n",
    "            # Also extract the actual text from context using offset\n",
    "            token_text_from_context = context[start_char:end_char]\n",
    "            found_answer_tokens.append({\n",
    "                'idx': token_idx,\n",
    "                'token': token,\n",
    "                'span': (start_char, end_char),\n",
    "                'context_text': token_text_from_context\n",
    "            })\n",
    "    print(found_answer_tokens)\n",
    "    if found_answer_tokens:\n",
    "        print(f\"\\n  Chunk {chunk_idx} - Found {len(found_answer_tokens)} answer tokens:\")\n",
    "        for token_info in found_answer_tokens:\n",
    "            print(f\"    Token {token_info['idx']}: '{token_info['token']}' \"\n",
    "                  f\"(chars {token_info['span'][0]}-{token_info['span'][1]}) \"\n",
    "                  f\"-> Context: '{token_info['context_text']}'\")\n",
    "        \n",
    "        # Reconstruct the full answer from tokens\n",
    "        answer_token_ids = [inputs['input_ids'][chunk_idx][t['idx']] for t in found_answer_tokens]\n",
    "        reconstructed_answer = tokenizer.decode(answer_token_ids)\n",
    "        print(f\"    Reconstructed answer: '{reconstructed_answer}'\")\n",
    "        print(f\"    Original answer:      '{answer[0]}'\")\n",
    "        print(f\"    Match: {reconstructed_answer.strip() == answer[0].strip().lower()}\")\n",
    "            \n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nKEY INSIGHT:\")\n",
    "print(\"The condition 'start_char < answer_end_char AND end_char > answer_start_char'\")\n",
    "print(\"ensures we capture ALL tokens overlapping with the answer, not just the first\")\n",
    "print(\"and last tokens. This is crucial for multi-token answers!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf33029",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf9d2c",
   "metadata": {},
   "source": [
    "### Process the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6dcc1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_data_preprocess(examples):\n",
    "    \"\"\"\n",
    "    generate start and end indexes of answer in context\n",
    "    \"\"\"\n",
    "    \n",
    "    def find_context_start_end_index(sequence_ids):\n",
    "        \"\"\"\n",
    "        returns the token index in which context starts and ends\n",
    "        \"\"\"\n",
    "        token_idx = 0\n",
    "        while sequence_ids[token_idx] != 1:  # means its special tokens or tokens of queston\n",
    "            token_idx += 1                   # loop only breask when context starts in tokens\n",
    "        context_start_idx = token_idx\n",
    "    \n",
    "        while sequence_ids[token_idx] == 1:\n",
    "            token_idx += 1\n",
    "        context_end_idx = token_idx - 1\n",
    "        return context_start_idx, context_end_idx  \n",
    "    \n",
    "    \n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    context = examples[\"context\"]\n",
    "    answers = examples[\"answers\"]\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        context,\n",
    "        max_length=512,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,  #returns id of base context\n",
    "        return_offsets_mapping=True,  # returns (start_index,end_index) of each token\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    for i, mapping_idx_pairs in enumerate(inputs['offset_mapping']):\n",
    "        context_idx = inputs['overflow_to_sample_mapping'][i]\n",
    "    \n",
    "        # from main context\n",
    "        answer = answers[context_idx]\n",
    "        answer_start_char_idx = answer['answer_start'][0]\n",
    "        # print(\"answer start char idx:\", answer_start_char_idx)\n",
    "        answer_end_char_idx = answer_start_char_idx + len(answer['text'][0])\n",
    "        # print(\"answer end char idx:\", answer_end_char_idx)\n",
    "\n",
    "        # now we have to find it in sub contexts\n",
    "        tokens = inputs['input_ids'][i]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        # print(\"sequence ids:\", sequence_ids)\n",
    "   \n",
    "        # finding the context start and end indexes wrt sub context tokens\n",
    "        context_start_idx, context_end_idx = find_context_start_end_index(sequence_ids)\n",
    "        # print(\"context start idx:\", context_start_idx)\n",
    "        # print(\"context end idx:\", context_end_idx)\n",
    "    \n",
    "        # if the answer is not fully inside context label it as (0,0)\n",
    "        # starting and end index of charecter of full context text\n",
    "        context_start_char_index = mapping_idx_pairs[context_start_idx][0]\n",
    "        # print(\"context start char index:\", context_start_char_index)\n",
    "        context_end_char_index = mapping_idx_pairs[context_end_idx][1]\n",
    "        # print(\"context end char index:\", context_end_char_index)\n",
    "    \n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if (context_start_char_index > answer_start_char_idx) or (\n",
    "            context_end_char_index < answer_end_char_idx):\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "    \n",
    "        else:\n",
    "            # else its start and end token positions\n",
    "            # here idx indicates index of token\n",
    "            idx = context_start_idx\n",
    "            # print(\"idx:\", idx)\n",
    "            while idx <= context_end_idx and mapping_idx_pairs[idx][0] <= answer_start_char_idx:\n",
    "                idx += 1\n",
    "            # print(\"final idx for start position:\", idx)\n",
    "            start_positions.append(idx - 1)  \n",
    "        \n",
    "            idx = context_end_idx\n",
    "            while idx >= context_start_idx and mapping_idx_pairs[idx][1] > answer_end_char_idx:\n",
    "                idx -= 1\n",
    "            # print(\"final idx for end position:\", idx)\n",
    "            end_positions.append(idx + 1)\n",
    "    \n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "    \n",
    "train_sample = dataset[\"train\"].select([i for i in range(100)])\n",
    "\n",
    "train_dataset = train_sample.map(\n",
    "    train_data_preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "len(dataset[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab23e63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'start_positions', 'end_positions'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b408ed",
   "metadata": {},
   "source": [
    "### Peek into the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f804acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "----\n",
      "Theoretical values :\n",
      " \n",
      "Question: \n",
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      " \n",
      "Context: \n",
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      " \n",
      "Answer: \n",
      "['Saint Bernadette Soubirous']\n",
      " \n",
      "Start and end index of text:  515 541\n",
      "--------------------------------------------------------------------------------\n",
      "Values after tokenization:\n",
      " \n",
      "Question: \n",
      "[CLS] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP]\n",
      " \n",
      "Context: \n",
      "architecturally, the school has a catholic character. atop the main building ' s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      " \n",
      "Answer: \n",
      "saint bernadette soubirous\n",
      " \n",
      "Start pos and end pos of tokens:  130 138\n",
      "________________________________________________________________________________\n",
      "1\n",
      "----\n",
      "Theoretical values :\n",
      " \n",
      "Question: \n",
      "What is in front of the Notre Dame Main Building?\n",
      " \n",
      "Context: \n",
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      " \n",
      "Answer: \n",
      "['a copper statue of Christ']\n",
      " \n",
      "Start and end index of text:  188 213\n",
      "--------------------------------------------------------------------------------\n",
      "Values after tokenization:\n",
      " \n",
      "Question: \n",
      "[CLS] what is in front of the notre dame main building? [SEP]\n",
      " \n",
      "Context: \n",
      "architecturally, the school has a catholic character. atop the main building ' s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      " \n",
      "Answer: \n",
      "a copper statue of christ\n",
      " \n",
      "Start pos and end pos of tokens:  52 57\n",
      "________________________________________________________________________________\n",
      "2\n",
      "----\n",
      "Theoretical values :\n",
      " \n",
      "Question: \n",
      "The Basilica of the Sacred heart at Notre Dame is beside to which structure?\n",
      " \n",
      "Context: \n",
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      " \n",
      "Answer: \n",
      "['the Main Building']\n",
      " \n",
      "Start and end index of text:  279 296\n",
      "--------------------------------------------------------------------------------\n",
      "Values after tokenization:\n",
      " \n",
      "Question: \n",
      "[CLS] the basilica of the sacred heart at notre dame is beside to which structure? [SEP]\n",
      " \n",
      "Context: \n",
      "architecturally, the school has a catholic character. atop the main building ' s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      " \n",
      "Answer: \n",
      "the main building\n",
      " \n",
      "Start pos and end pos of tokens:  81 84\n",
      "________________________________________________________________________________\n",
      "3\n",
      "----\n",
      "Theoretical values :\n",
      " \n",
      "Question: \n",
      "What is the Grotto at Notre Dame?\n",
      " \n",
      "Context: \n",
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      " \n",
      "Answer: \n",
      "['a Marian place of prayer and reflection']\n",
      " \n",
      "Start and end index of text:  381 420\n",
      "--------------------------------------------------------------------------------\n",
      "Values after tokenization:\n",
      " \n",
      "Question: \n",
      "[CLS] what is the grotto at notre dame? [SEP]\n",
      " \n",
      "Context: \n",
      "architecturally, the school has a catholic character. atop the main building ' s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      " \n",
      "Answer: \n",
      "a marian place of prayer and reflection\n",
      " \n",
      "Start pos and end pos of tokens:  95 102\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def print_context_and_answer(idx):\n",
    "    \n",
    "    # original data\n",
    "    print(idx)\n",
    "    print('----')\n",
    "    question = dataset[\"train\"][idx]['question']\n",
    "    context = dataset[\"train\"][idx]['context']\n",
    "    answer = dataset[\"train\"][idx]['answers']['text']\n",
    "    print('Theoretical values :')\n",
    "    print(' ')\n",
    "    print('Question: ')\n",
    "    print(question)\n",
    "    print(' ')\n",
    "    print('Context: ')\n",
    "    print(context)\n",
    "    print(' ')\n",
    "    print('Answer: ')\n",
    "    print(answer)\n",
    "    print(' ')\n",
    "    answer_start_char_idx = dataset[\"train\"][idx]['answers']['answer_start'][0]\n",
    "    answer_end_char_idx = answer_start_char_idx + len(dataset[\"train\"][idx]['answers']['text'][0])\n",
    "    print('Start and end index of text: ',answer_start_char_idx,answer_end_char_idx)\n",
    "    print('----'*20)\n",
    "\n",
    "    # Mapped Data\n",
    "    print('Values after tokenization:')\n",
    "    sep_tok_index = train_dataset[idx]['input_ids'].index(102) #get index for [SEP]\n",
    "    question_ = train_dataset[idx]['input_ids'][:sep_tok_index+1]\n",
    "    question_decoded = tokenizer.decode(question_) \n",
    "    context_ = train_dataset[idx]['input_ids'][sep_tok_index+1:]\n",
    "    context_decoded = tokenizer.decode(context_) \n",
    "    start_idx = train_dataset[idx]['start_positions']\n",
    "    end_idx = train_dataset[idx]['end_positions']\n",
    "    answer_toks = train_dataset[idx]['input_ids'][start_idx:end_idx]\n",
    "    answer_decoded = tokenizer.decode(answer_toks)\n",
    "    print(' ')\n",
    "    print('Question: ')\n",
    "    print(question_decoded)\n",
    "    print(' ')\n",
    "    print('Context: ')\n",
    "    print(context_decoded)\n",
    "    print(' ')\n",
    "    print('Answer: ')\n",
    "    print(answer_decoded)\n",
    "    print(' ')\n",
    "    print('Start pos and end pos of tokens: ',train_dataset[idx]['start_positions'],train_dataset[idx]['end_positions'])\n",
    "    print('____'*20)\n",
    "    \n",
    "    \n",
    "print_context_and_answer(0)\n",
    "print_context_and_answer(1)\n",
    "print_context_and_answer(2)\n",
    "print_context_and_answer(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54ca2e4",
   "metadata": {},
   "source": [
    "### Process the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d3fd2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    \"\"\"\n",
    "    preprocessing validation data\n",
    "    \"\"\"\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    context = examples[\"context\"]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        context,\n",
    "        max_length=512,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    base_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        \n",
    "        # take the base id (ie in cases of overflow happens we get base id)\n",
    "        base_context_idx = sample_map[i]\n",
    "        base_ids.append(examples[\"id\"][base_context_idx])\n",
    "        \n",
    "        # sequence id indicates the input. 0 for first input and 1 for second input\n",
    "        # and None for special tokens by default\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        \n",
    "        # for Question tokens provide offset_mapping as None\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"base_id\"] = base_ids\n",
    "    return inputs\n",
    "\n",
    "\n",
    "val_sample = dataset[\"validation\"].select([i for i in range(100)])\n",
    "\n",
    "eval_set = val_sample.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    ")\n",
    "len(eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0beb6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'base_id'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3693f3e6",
   "metadata": {},
   "source": [
    "# Model Performance on eval set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15cd81b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column names of eval set for model:  ['input_ids', 'attention_mask']\n",
      "Batch shapes:\n",
      "  input_ids: torch.Size([100, 512])\n",
      "  attention_mask: torch.Size([100, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((100, 512), (100, 512))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set_for_model = eval_set.remove_columns([\"base_id\", \"offset_mapping\"])\n",
    "print(\"column names of eval set for model: \", eval_set_for_model.column_names)\n",
    "eval_set_for_model.set_format(\"torch\")\n",
    "\n",
    "# Method 1: Stack all examples into a single batch (works for small datasets)\n",
    "# Need to stack the list of tensors into a single batched tensor\n",
    "batch = {k: torch.stack([eval_set_for_model[i][k] for i in range(len(eval_set_for_model))]) \n",
    "         for k in eval_set_for_model.column_names}\n",
    "\n",
    "# Move batch to device\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "print(\"Batch shapes:\")\n",
    "for k, v in batch.items():\n",
    "    print(f\"  {k}: {v.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    \n",
    "start_logits = outputs.start_logits.cpu().numpy()\n",
    "end_logits = outputs.end_logits.cpu().numpy()\n",
    "\n",
    "start_logits.shape, end_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496464b",
   "metadata": {},
   "source": [
    "## Calculate metrics for un-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2285b94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.0, 'f1': 4.876809727267394}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "def predict_answers_and_evaluate(start_logits, end_logits, eval_set, examples):\n",
    "    \"\"\"\n",
    "    make predictions \n",
    "    Args:\n",
    "    start_logits : strat_position prediction logits\n",
    "    end_logits: end_position prediction logits\n",
    "    eval_set: processed val data\n",
    "    examples: unprocessed val data with context text\n",
    "    \"\"\"\n",
    "    # appending all id's corresponding to the base context id\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "\n",
    "    for idx, feature in enumerate(eval_set):\n",
    "        example_to_features[feature[\"base_id\"]].append(idx)\n",
    "\n",
    "    n_best = 20\n",
    "    max_answer_length = 30\n",
    "    predicted_answers = []\n",
    "\n",
    "    for example in examples:\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # looping through each sub contexts corresponding to a context and finding\n",
    "        # answers\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            \n",
    "            offsets = eval_set[\"offset_mapping\"][feature_index]\n",
    "        \n",
    "            # sorting the predictions of all hidden states and taking best n_best prediction\n",
    "            # means taking the index of top 20 tokens\n",
    "            start_indexes = np.argsort(start_logit).tolist()[::-1][:n_best]\n",
    "            end_indexes = np.argsort(end_logit).tolist()[::-1][:n_best]\n",
    "        \n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                \n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                       ):\n",
    "                        continue\n",
    "\n",
    "                    answers.append({\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                        })\n",
    "\n",
    "\n",
    "    \n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    metric = evaluate.load(\"squad\")\n",
    "\n",
    "    theoretical_answers = [\n",
    "            {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples\n",
    "    ]\n",
    "    \n",
    "    metric_ = metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
    "    return predicted_answers, metric_\n",
    "\n",
    "pred_answers,metrics_ = predict_answers_and_evaluate(start_logits, end_logits, eval_set, val_sample)\n",
    "metrics_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d74b69",
   "metadata": {},
   "source": [
    "# Training a Question Answering System based on BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e974f",
   "metadata": {},
   "source": [
    "## Preparing data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "366cb880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets sample a small dataset\n",
    "dataset['train'] = dataset['train'].select([i for i in range(10000)])\n",
    "dataset['validation'] = dataset['validation'].select([i for i in range(1000)])\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a03a9403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class DataQA(Dataset):\n",
    "    def __init__(self, dataset, mode=\"train\"):\n",
    "        self.mode = mode\n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "            # sampling\n",
    "            self.dataset = dataset[\"train\"]\n",
    "            self.data = self.dataset.map(train_data_preprocess,\n",
    "                                        batched=True,\n",
    "                                        remove_columns= dataset[\"train\"].column_names\n",
    "                                    )\n",
    "        \n",
    "        else:\n",
    "            self.dataset = dataset[\"validation\"]\n",
    "            self.data = self.dataset.map(preprocess_validation_examples,\n",
    "                                    batched=True,\n",
    "                                    remove_columns = dataset[\"validation\"].column_names     \n",
    "                                 )\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        out = {}\n",
    "        example = self.data[idx]\n",
    "        out['input_ids'] = torch.tensor(example['input_ids'])\n",
    "        out['attention_mask'] = torch.tensor(example['attention_mask'])\n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "            out['start_positions'] = torch.unsqueeze(torch.tensor(example['start_positions']), dim=0)\n",
    "            out['end_positions'] = torch.unsqueeze(torch.tensor(example['end_positions']), dim=0)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f146e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04263d8abaa42a286727b8a1470555c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = DataQA(dataset, mode=\"train\")\n",
    "val_dataset = DataQA(dataset, mode=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "264359f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Sample Shapes:\n",
      "\n",
      "input_ids :  torch.Size([512])\n",
      "attention_mask :  torch.Size([512])\n",
      "start_positions :  torch.Size([1])\n",
      "end_positions :  torch.Size([1])\n",
      "--------------------------------------------------------------------------------\n",
      "input_ids :  torch.Size([512])\n",
      "attention_mask :  torch.Size([512])\n",
      "start_positions :  torch.Size([1])\n",
      "end_positions :  torch.Size([1])\n",
      "--------------------------------------------------------------------------------\n",
      "input_ids :  torch.Size([512])\n",
      "attention_mask :  torch.Size([512])\n",
      "start_positions :  torch.Size([1])\n",
      "end_positions :  torch.Size([1])\n",
      "--------------------------------------------------------------------------------\n",
      "____________________________________________________________________________________________________\n",
      "Validation Dataset Sample Lengths:\n",
      "\n",
      "input_ids :  torch.Size([512])\n",
      "attention_mask :  torch.Size([512])\n",
      "--------------------------------------------------------------------------------\n",
      "input_ids :  torch.Size([512])\n",
      "attention_mask :  torch.Size([512])\n",
      "--------------------------------------------------------------------------------\n",
      "input_ids :  torch.Size([512])\n",
      "attention_mask :  torch.Size([512])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Dataset Sample Shapes:\\n\")\n",
    "for i, d in enumerate(train_dataset):\n",
    "    for k in d.keys():\n",
    "        print(k + ' : ', d[k].shape)\n",
    "    print('--'*40)\n",
    "\n",
    "    if i == 2:\n",
    "        break\n",
    "        \n",
    "print('__'*50)\n",
    "\n",
    "print(\"Validation Dataset Sample Lengths:\\n\")\n",
    "for i,d in enumerate(val_dataset):\n",
    "    for k in d.keys():\n",
    "        print(k + ' : ', d[k].shape)\n",
    "    print('--'*40)\n",
    "    \n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8557f227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 1])\n",
      "------------------------------------------------------------\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=2,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    collate_fn=default_data_collator, \n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "   print(batch['input_ids'].shape)\n",
    "   print(batch['attention_mask'].shape)\n",
    "   print(batch['start_positions'].shape)\n",
    "   print(batch['end_positions'].shape)\n",
    "   break\n",
    "\n",
    "print('---'*20)\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "   print(batch['input_ids'].shape)\n",
    "   print(batch['attention_mask'].shape)\n",
    "   break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0660100",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c023a3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20020\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "print(total_steps)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9c1beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need processed validation data to get offsets at the time of evaluation\n",
    "validation_processed_dataset = dataset[\"validation\"].map(\n",
    "        preprocess_validation_examples,\n",
    "        batched=True,\n",
    "        remove_columns = dataset[\"validation\"].column_names,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2cf38c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "=====Epoch 1=====\n",
      "Training....\n",
      "  Batch 1,000  of  5,005.    Elapsed: 0:31:22.\n",
      "  Batch 1000/5005 - Loss: 3.0481\n",
      "  Batch 2,000  of  5,005.    Elapsed: 1:00:30.\n",
      "  Batch 2000/5005 - Loss: 2.1065\n",
      "  Batch 3,000  of  5,005.    Elapsed: 1:33:02.\n",
      "  Batch 3000/5005 - Loss: 1.9082\n",
      "  Batch 4,000  of  5,005.    Elapsed: 2:02:39.\n",
      "  Batch 4000/5005 - Loss: 1.6742\n",
      "  Batch 5,000  of  5,005.    Elapsed: 2:33:53.\n",
      "  Batch 5000/5005 - Loss: 1.5530\n",
      "\n",
      "  Average training loss: 2.06\n",
      "  Training epoch took: 2:34:01\n",
      "\n",
      "Running Validation...\n",
      "Exact match: 26.7, F1 score: 62.92895154304274\n",
      "\n",
      "  Validation took: 0:03:14\n",
      " \n",
      "=====Epoch 2=====\n",
      "Training....\n",
      "  Batch 1,000  of  5,005.    Elapsed: 0:29:22.\n",
      "  Batch 1000/5005 - Loss: 1.0126\n",
      "  Batch 2,000  of  5,005.    Elapsed: 0:58:31.\n",
      "  Batch 2000/5005 - Loss: 1.0061\n",
      "  Batch 3,000  of  5,005.    Elapsed: 1:43:54.\n",
      "  Batch 3000/5005 - Loss: 0.9847\n",
      "  Batch 4,000  of  5,005.    Elapsed: 2:24:42.\n",
      "  Batch 4000/5005 - Loss: 0.9739\n",
      "  Batch 5,000  of  5,005.    Elapsed: 2:54:49.\n",
      "  Batch 5000/5005 - Loss: 1.0149\n",
      "\n",
      "  Average training loss: 1.00\n",
      "  Training epoch took: 2:54:57\n",
      "\n",
      "Running Validation...\n",
      "Exact match: 27.5, F1 score: 64.38793258977005\n",
      "\n",
      "  Validation took: 0:03:27\n",
      " \n",
      "=====Epoch 3=====\n",
      "Training....\n",
      "  Batch 1,000  of  5,005.    Elapsed: 0:29:55.\n",
      "  Batch 1000/5005 - Loss: 0.4938\n",
      "  Batch 2,000  of  5,005.    Elapsed: 0:59:32.\n",
      "  Batch 2000/5005 - Loss: 0.6193\n",
      "  Batch 3,000  of  5,005.    Elapsed: 1:29:07.\n",
      "  Batch 3000/5005 - Loss: 0.5644\n",
      "  Batch 4,000  of  5,005.    Elapsed: 1:58:21.\n",
      "  Batch 4000/5005 - Loss: 0.5784\n",
      "  Batch 5,000  of  5,005.    Elapsed: 2:28:21.\n",
      "  Batch 5000/5005 - Loss: 0.5607\n",
      "\n",
      "  Average training loss: 0.56\n",
      "  Training epoch took: 2:28:30\n",
      "\n",
      "Running Validation...\n",
      "Exact match: 28.7, F1 score: 64.7625193490258\n",
      "\n",
      "  Validation took: 0:03:13\n",
      " \n",
      "=====Epoch 4=====\n",
      "Training....\n",
      "  Batch 1,000  of  5,005.    Elapsed: 0:29:29.\n",
      "  Batch 1000/5005 - Loss: 0.3084\n",
      "  Batch 2,000  of  5,005.    Elapsed: 0:59:17.\n",
      "  Batch 2000/5005 - Loss: 0.3693\n",
      "  Batch 3,000  of  5,005.    Elapsed: 1:28:58.\n",
      "  Batch 3000/5005 - Loss: 0.3866\n",
      "  Batch 4,000  of  5,005.    Elapsed: 1:58:32.\n",
      "  Batch 4000/5005 - Loss: 0.3855\n",
      "  Batch 5,000  of  5,005.    Elapsed: 2:27:50.\n",
      "  Batch 5000/5005 - Loss: 0.3819\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 2:27:58\n",
      "\n",
      "Running Validation...\n",
      "Exact match: 28.2, F1 score: 64.98233777424922\n",
      "\n",
      "  Validation took: 0:03:12\n",
      "\n",
      "Training complete!\n",
      "Total training took 10:38:33 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random,time\n",
    "\n",
    "# to reproduce results\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "# torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "#storing all training and validation stats\n",
    "stats = []\n",
    "\n",
    "#to measure total training time\n",
    "total_train_time_start = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(' ')\n",
    "    print(f'=====Epoch {epoch + 1}=====')\n",
    "    print('Training....')\n",
    "     \n",
    "    # ===============================\n",
    "    #    Train\n",
    "    # ===============================   \n",
    "    # measure how long training epoch takes\n",
    "    t0 = time.time()\n",
    "     \n",
    "    training_loss = 0\n",
    "    plot_loss = 0\n",
    "    # loop through train data\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "         \n",
    "        # we will print train time in aftere each 1000 batch\n",
    "        if step%1000 == 0 and not step == 0:\n",
    "              elapsed_time = format_time(time.time() - t0)\n",
    "              # Report progress.\n",
    "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed_time))\n",
    "\n",
    "         \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "            \n",
    "        #set gradients to zero\n",
    "        model.zero_grad()\n",
    "\n",
    "        result = model(input_ids = input_ids, \n",
    "                        attention_mask = attention_mask,\n",
    "                        start_positions = start_positions,\n",
    "                        end_positions = end_positions,\n",
    "                        return_dict=True)\n",
    "         \n",
    "        loss = result.loss\n",
    "    \n",
    "        # accumulate the loss over batches so that we can calculate avg loss at the end\n",
    "        training_loss += loss.item()\n",
    "        plot_loss += loss.item()\n",
    "        \n",
    "        # We will print train loss after 1000 batches\n",
    "        if step%1000 == 0 and not step == 0:\n",
    "            avg_loss = plot_loss/1000\n",
    "            print(f'  Batch {step}/{len(train_dataloader)} - Loss: {avg_loss:.4f}')\n",
    "            plot_loss = 0\n",
    "\n",
    "        # perform backward propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # update the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "    # calculate avg loss\n",
    "    avg_train_loss = training_loss/len(train_dataloader) \n",
    " \n",
    "    # calculates training time\n",
    "    training_time = format_time(time.time() - t0)\n",
    "     \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "    \n",
    "    # ===============================\n",
    "    #    Validation\n",
    "    # ===============================\n",
    "     \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    start_logits, end_logits = [],[]\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "    \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():  \n",
    "             result = model(\n",
    "                    input_ids = input_ids, \n",
    "                    attention_mask = attention_mask,\n",
    "                    return_dict=True\n",
    "                    )\n",
    "        \n",
    "        start_logits.append(result.start_logits.cpu().numpy())\n",
    "        end_logits.append(result.end_logits.cpu().numpy())\n",
    "   \n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "\n",
    "    # calculating metrics\n",
    "    answers,metrics_ = predict_answers_and_evaluate(start_logits, end_logits, validation_processed_dataset, dataset[\"validation\"])\n",
    "    print(f'Exact match: {metrics_[\"exact_match\"]}, F1 score: {metrics_[\"f1\"]}')\n",
    "\n",
    "    print('')\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_train_time_start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708301ef",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa931596",
   "metadata": {},
   "source": [
    "## Method 1: Produce multilple answers and select best based on logits, handle large contexts too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8364c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_long_sequence(question, answer_text):\n",
    "    '''\n",
    "    Takes a `question` string and an `answer_text` string (which contains the\n",
    "    answer), and identifies the words within the `answer_text` that are the\n",
    "    answer. Prints them out.\n",
    "    \n",
    "    Handles both short and long contexts by processing multiple chunks if needed.\n",
    "    '''\n",
    "    # ======== Tokenize ========\n",
    "    # Apply the tokenizer to the input text with overflow handling\n",
    "    encoded_dict = tokenizer(\n",
    "                    question, \n",
    "                    answer_text,\n",
    "                    max_length=512,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=\"only_second\",\n",
    "                    stride=128,\n",
    "                    return_overflowing_tokens=True,\n",
    "                    return_offsets_mapping=True, \n",
    "                )\n",
    "    \n",
    "    # Get all chunks\n",
    "    input_ids_list = encoded_dict['input_ids']\n",
    "    attention_mask_list = encoded_dict['attention_mask']\n",
    "    offset_mapping_list = encoded_dict['offset_mapping']\n",
    "    \n",
    "    print(f'Context split into {len(input_ids_list)} chunk(s)\\n')\n",
    "    \n",
    "    # ======== Process Each Chunk ========\n",
    "    best_answer = None\n",
    "    best_score = float('-inf')\n",
    "    \n",
    "    for chunk_idx in range(len(input_ids_list)):\n",
    "        input_ids = input_ids_list[chunk_idx]\n",
    "        attention_mask = attention_mask_list[chunk_idx]\n",
    "        offsets = offset_mapping_list[chunk_idx]\n",
    "        \n",
    "        # Get sequence IDs to identify context tokens\n",
    "        sequence_ids = encoded_dict.sequence_ids(chunk_idx)\n",
    "        \n",
    "        # Run model on this chunk\n",
    "        with torch.no_grad():\n",
    "            output = model(\n",
    "                torch.tensor([input_ids]).to(device),\n",
    "                attention_mask=torch.tensor([attention_mask]).to(device)\n",
    "            )\n",
    "        \n",
    "        start_logits = output.start_logits[0].cpu().numpy()\n",
    "        end_logits = output.end_logits[0].cpu().numpy()\n",
    "        \n",
    "        # Find top N predictions for this chunk\n",
    "        n_best = 5\n",
    "        max_answer_length = 30\n",
    "        \n",
    "        start_indexes = np.argsort(start_logits).tolist()[::-1][:n_best]\n",
    "        end_indexes = np.argsort(end_logits).tolist()[::-1][:n_best]\n",
    "        \n",
    "        # Try all combinations of start and end positions\n",
    "        for start_idx in start_indexes:\n",
    "            for end_idx in end_indexes:\n",
    "                # Skip if not in context (sequence_id should be 1 for context)\n",
    "                if sequence_ids[start_idx] != 1 or sequence_ids[end_idx] != 1:\n",
    "                    continue\n",
    "                    \n",
    "                # Skip invalid spans\n",
    "                if end_idx < start_idx or end_idx - start_idx + 1 > max_answer_length:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate score\n",
    "                score = start_logits[start_idx] + end_logits[end_idx]\n",
    "                \n",
    "                # Extract answer text using offsets\n",
    "                start_char = offsets[start_idx][0]\n",
    "                end_char = offsets[end_idx][1]\n",
    "                answer_candidate = answer_text[start_char:end_char]\n",
    "                \n",
    "                # Update best answer if this is better\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_answer = {\n",
    "                        'text': answer_candidate,\n",
    "                        'score': score,\n",
    "                        'chunk': chunk_idx,\n",
    "                        'start_idx': start_idx,\n",
    "                        'end_idx': end_idx\n",
    "                    }\n",
    "    \n",
    "    # ======== Display Result ========\n",
    "    if best_answer:\n",
    "        print(f'Best Answer: \"{best_answer[\"text\"]}\"')\n",
    "        print(f'Confidence Score: {best_answer[\"score\"]:.4f}')\n",
    "        print(f'Found in chunk: {best_answer[\"chunk\"] + 1}/{len(input_ids_list)}')\n",
    "    else:\n",
    "        print('No valid answer found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844e459b",
   "metadata": {},
   "source": [
    "## Method 2: Simple answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f1bef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, answer_text):\n",
    "    '''\n",
    "    Takes a `question` string and an `answer_text` string (which contains the\n",
    "    answer), and identifies the words within the `answer_text` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "    # ======== Tokenize ========\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    encoded_dict = tokenizer(\n",
    "                    question, \n",
    "                    answer_text,      # Sentence to encode.\n",
    "                    add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n",
    "                    max_length=512,\n",
    "                    padding=\"max_length\",\n",
    "                )\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "\n",
    "    assert len(attention_mask) == len(input_ids)\n",
    "\n",
    "    # Report how long the input sequence is.\n",
    "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "    # ======== Evaluate ========\n",
    "    # Run our example question through the model.\n",
    "    output = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                    attention_mask=torch.tensor([attention_mask])) # The attention mask to differentiate question from answer_text\n",
    "    \n",
    "    start_scores = output.start_logits\n",
    "    end_scores = output.end_logits\n",
    "\n",
    "    # ======== Reconstruct Answer ========\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the string versions of the input tokens.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    print('Simple answer generation : \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "550a99f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  How many parameters does BERT-large have?\n",
      "Query has 512 tokens.\n",
      "\n",
      "Simple answer generation : \"340m parameters\"\n",
      "Context split into 1 chunk(s)\n",
      "\n",
      "Best Answer: \"340M parameters\"\n",
      "Confidence Score: 18.5874\n",
      "Found in chunk: 1/1\n"
     ]
    }
   ],
   "source": [
    "question = \"How many parameters does BERT-large have?\"\n",
    "bert_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB,so expect it to take a couple minutes to download to your Colab instance.\"\n",
    "\n",
    "print(\"Question: \", question)\n",
    "answer_question(question, bert_text)\n",
    "answer_question_long_sequence(question, bert_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "313480d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What are some example applications of BERT?\n",
      "Query has 512 tokens.\n",
      "\n",
      "Simple answer generation : \"cola\"\n",
      "Context split into 1 chunk(s)\n",
      "\n",
      "Best Answer: \"it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether\"\n",
      "Confidence Score: -6.6550\n",
      "Found in chunk: 1/1\n"
     ]
    }
   ],
   "source": [
    "question = \"What are some example applications of BERT?\"\n",
    "\n",
    "print(\"Question: \", question)\n",
    "answer_question(question, bert_text)\n",
    "answer_question_long_sequence(question, bert_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ed2408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What does the 'B' in BERT stand for?\n",
      "Query has 512 tokens.\n",
      "\n",
      "Simple answer generation : \"it\"\n",
      "Context split into 1 chunk(s)\n",
      "\n",
      "Best Answer: \"large is\"\n",
      "Confidence Score: -3.7095\n",
      "Found in chunk: 1/1\n"
     ]
    }
   ],
   "source": [
    "question = \"What does the 'B' in BERT stand for?\"\n",
    "\n",
    "print(\"Question: \", question)\n",
    "answer_question(question, bert_text)\n",
    "answer_question_long_sequence(question, bert_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d64e177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is my name?\n",
      "Query has 512 tokens.\n",
      "\n",
      "Simple answer generation : \"tushar .\"\n",
      "Context split into 1 chunk(s)\n",
      "\n",
      "Best Answer: \"Tushar.\"\n",
      "Confidence Score: 13.8101\n",
      "Found in chunk: 1/1\n"
     ]
    }
   ],
   "source": [
    "question = \"What is my name?\"\n",
    "context = \"My name is Tushar. My job is of data scientist\"\n",
    "\n",
    "print(\"Question: \", question)\n",
    "answer_question(question, context)\n",
    "answer_question_long_sequence(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38ad9472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is my job?\n",
      "Query has 512 tokens.\n",
      "\n",
      "Simple answer generation : \"data\"\n",
      "Context split into 1 chunk(s)\n",
      "\n",
      "Best Answer: \"Tushar.\"\n",
      "Confidence Score: 6.2145\n",
      "Found in chunk: 1/1\n"
     ]
    }
   ],
   "source": [
    "question = \"What is my job?\"\n",
    "\n",
    "print(\"Question: \", question)\n",
    "answer_question(question, context)\n",
    "answer_question_long_sequence(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7659bd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Which NFL team represented the AFC at Super Bowl 50?\n",
      "Query has 512 tokens.\n",
      "\n",
      "Simple answer generation : \"denver broncos defeated\"\n",
      "Context split into 1 chunk(s)\n",
      "\n",
      "Best Answer: \"Denver Broncos defeated\"\n",
      "Confidence Score: 18.4528\n",
      "Found in chunk: 1/1\n"
     ]
    }
   ],
   "source": [
    "question = \"Which NFL team represented the AFC at Super Bowl 50?\"\n",
    "answer_text = \"\"\"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) \\\n",
    "            champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, \\\n",
    "            2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various \\\n",
    "            gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\n",
    "            \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\"\"\"\n",
    "\n",
    "print(\"Question: \", question)\n",
    "answer_question(question, answer_text)\n",
    "answer_question_long_sequence(question, answer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e7cf3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Query has 512 tokens.\n",
      "\n",
      "Simple answer generation : \"saint bernadette soubirous in\"\n",
      "Context split into 1 chunk(s)\n",
      "\n",
      "Best Answer: \"Saint Bernadette Soubirous in\"\n",
      "Confidence Score: 20.3800\n",
      "Found in chunk: 1/1\n"
     ]
    }
   ],
   "source": [
    "question = \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\"\n",
    " \n",
    "context = \"\"\"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. \\\n",
    "    Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". \\\n",
    "        Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. \\\n",
    "            It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. \\\n",
    "                At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\"\"\"\n",
    "\n",
    "print(\"Question: \", question)\n",
    "answer_question(question, context)\n",
    "answer_question_long_sequence(question, context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef623502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Query has 512 tokens.\n",
      "\n",
      "Simple answer generation : \"saint bernadette soubirous in\"\n",
      "Context split into 1 chunk(s)\n",
      "\n",
      "Best Answer: \"Saint Bernadette Soubirous in\"\n",
      "Confidence Score: 20.3800\n",
      "Found in chunk: 1/1\n"
     ]
    }
   ],
   "source": [
    "questions = 'What is in front of the Notre Dame Main Building?'\n",
    "\n",
    "print(\"Question: \", question)\n",
    "answer_question(question, context)\n",
    "answer_question_long_sequence(question, context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject-ker",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
