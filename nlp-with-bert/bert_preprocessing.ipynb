{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebc39904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 05:30:11.137910: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db0eb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.15.1\n",
      "Tensorflow Text version: 2.15.0\n",
      "Python 3.11.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"Tensorflow Text version:\", text.__version__)\n",
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e538587c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_a': <tf.Tensor: shape=(), dtype=string, numpy=b'Sponge bob Squarepants is an Avenger'>,\n",
       " 'text_b': <tf.Tensor: shape=(), dtype=string, numpy=b'Barack Obama is the President.'>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = {\n",
    "    \"text_a\": [\n",
    "      \"Sponge bob Squarepants is an Avenger\",\n",
    "      \"Marvel Avengers\"\n",
    "    ],\n",
    "    \"text_b\": [\n",
    "     \"Barack Obama is the President.\",\n",
    "     \"President is the highest office\"\n",
    "  ],\n",
    "}\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(examples)\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bfdd6f",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f59dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "_VOCAB = [\n",
    "    # Special tokens\n",
    "    b\"[UNK]\", b\"[MASK]\", b\"[RANDOM]\", b\"[CLS]\", b\"[SEP]\",\n",
    "    # Suffixes\n",
    "    b\"##ack\", b\"##ama\", b\"##ger\", b\"##gers\", b\"##onge\", b\"##pants\",  b\"##uare\",\n",
    "    b\"##vel\", b\"##ven\", b\"an\", b\"A\", b\"Bar\", b\"Hates\", b\"Mar\", b\"Ob\",\n",
    "    b\"Patrick\", b\"President\", b\"Sp\", b\"Sq\", b\"bob\", b\"box\", b\"has\", b\"highest\",\n",
    "    b\"is\", b\"office\", b\"the\",\n",
    "]\n",
    "\n",
    "_START_TOKEN = _VOCAB.index(b\"[CLS]\")\n",
    "_END_TOKEN = _VOCAB.index(b\"[SEP]\")\n",
    "_MASK_TOKEN = _VOCAB.index(b\"[MASK]\")\n",
    "_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\n",
    "_UNK_TOKEN = _VOCAB.index(b\"[UNK]\")\n",
    "_MAX_SEQ_LEN = 8\n",
    "_MAX_PREDICTIONS_PER_BATCH = 5\n",
    "\n",
    "_VOCAB_SIZE = len(_VOCAB)\n",
    "\n",
    "lookup_table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "      keys=_VOCAB,\n",
    "      key_dtype=tf.string,\n",
    "      values=tf.range(\n",
    "          tf.size(_VOCAB, out_type=tf.int64), dtype=tf.int64),\n",
    "          value_dtype=tf.int64\n",
    "        ),\n",
    "      num_oov_buckets=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7997a0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(31,), dtype=int64, numpy=\n",
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_table.lookup(tf.constant(_VOCAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f16e2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Text:\n",
      " ['Sponge bob Squarepants is an Avenger', 'Marvel Avengers']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[b'Sp', b'##onge'], [b'bob'], [b'Sq', b'##uare', b'##pants'], [b'is'],\n",
       "  [b'an'], [b'A', b'##ven', b'##ger']]                                  ,\n",
       " [[b'Mar', b'##vel'], [b'A', b'##ven', b'##gers']]]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer = text.BertTokenizer(lookup_table, token_out_type=tf.string)\n",
    "print(\"Example Text:\\n\", examples[\"text_a\"] )\n",
    "bert_tokenizer.tokenize(examples[\"text_a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76762d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Text:\n",
      " ['Barack Obama is the President.', 'President is the highest office']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[b'Bar', b'##ack'], [b'Ob', b'##ama'], [b'is'], [b'the'], [b'President'],\n",
       "  [b'[UNK]']]                                                              ,\n",
       " [[b'President'], [b'is'], [b'the'], [b'highest'], [b'office']]]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Example Text:\\n\", examples[\"text_b\"] )\n",
    "bert_tokenizer.tokenize(examples[\"text_b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd7268a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[22, 9], [24], [23, 11, 10], [28], [14], [15, 13, 7]],\n",
       " [[18, 12], [15, 13, 8]]]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer = text.BertTokenizer(lookup_table, token_out_type=tf.int64)\n",
    "segment_a = bert_tokenizer.tokenize(examples[\"text_a\"])\n",
    "segment_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "607815a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[16, 5], [19, 6], [28], [30], [21], [0]], [[21], [28], [30], [27], [29]]]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_b = bert_tokenizer.tokenize(examples[\"text_b\"])\n",
    "segment_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01dc3def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, None, None])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afbf3b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[22, 9, 24, 23, 11, 10, 28, 14, 15, 13, 7], [18, 12, 15, 13, 8]]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_a = segment_a.merge_dims(-2, -1)\n",
    "segment_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d89b9f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, None, None])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fabff1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[16, 5, 19, 6, 28, 30, 21, 0], [21, 28, 30, 27, 29]]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_b = segment_b.merge_dims(-2, -1)\n",
    "segment_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4cafc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.RaggedTensor [[22, 9, 24, 23],\n",
       "  [18, 12, 15, 13]]>,\n",
       " <tf.RaggedTensor [[16, 5, 19, 6],\n",
       "  [21, 28, 30, 27]]>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer = text.RoundRobinTrimmer(max_seq_length=_MAX_SEQ_LEN)\n",
    "trimmed = trimmer.trim([segment_a, segment_b])\n",
    "trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f6d2c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, None]), TensorShape([2, None]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed[0].shape, trimmed[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c602dcb",
   "metadata": {},
   "source": [
    "# Combining segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d658d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.RaggedTensor [[3, 22, 9, 24, 23, 4, 16, 5, 19, 6, 4],\n",
       "  [3, 18, 12, 15, 13, 4, 21, 28, 30, 27, 4]]>,\n",
       " <tf.RaggedTensor [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments_combined, segments_ids = text.combine_segments(\n",
    "  trimmed,\n",
    "  start_of_sequence_id=_START_TOKEN, end_of_segment_id=_END_TOKEN)\n",
    "segments_combined, segments_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d8ff2",
   "metadata": {},
   "source": [
    "# Masked Language Model Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705dba39",
   "metadata": {},
   "source": [
    "## Item Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f77819c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[False, True, False, False, False, False, False, False, False, True,\n",
       "  False],\n",
       " [False, False, True, False, False, False, False, False, False, True,\n",
       "  False]]>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_selector = text.RandomItemSelector(\n",
    "    max_selections_per_batch=_MAX_PREDICTIONS_PER_BATCH,\n",
    "    selection_rate=0.2,\n",
    "    unselectable_ids=[_START_TOKEN, _END_TOKEN, _UNK_TOKEN]\n",
    ")\n",
    "selected = random_selector.get_selection_mask(\n",
    "    segments_combined, axis=1)\n",
    "selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba931b2",
   "metadata": {},
   "source": [
    "## Choosing the Masked Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b041e5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[1, 1, 1, 1, 1, 18, 1, 1, 1, 6, 1],\n",
       " [1, 1, 1, 1, 1, 4, 1, 1, 1, 3, 3]]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_values_chooser = text.MaskValuesChooser(_VOCAB_SIZE, _MASK_TOKEN, 0.8)\n",
    "mask_values_chooser.get_mask_values(segments_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb99f05f",
   "metadata": {},
   "source": [
    "## Generating Inputs for Masked Language Model Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5c37f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_token_ids, masked_pos, masked_lm_ids = text.mask_language_model(\n",
    "  segments_combined,\n",
    "  item_selector=random_selector, mask_values_chooser=mask_values_chooser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32b30c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[3, 22, 1, 24, 23, 4, 1, 5, 19, 6, 4],\n",
       " [3, 18, 1, 15, 13, 4, 21, 28, 30, 1, 4]]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ef586a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[CLS]', b'Sp', b'##onge', b'bob', b'Sq', b'[SEP]', b'Bar', b'##ack',\n",
       "  b'Ob', b'##ama', b'[SEP]'],\n",
       " [b'[CLS]', b'Mar', b'##vel', b'A', b'##ven', b'[SEP]', b'President',\n",
       "  b'is', b'the', b'highest', b'[SEP]']]>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(_VOCAB, segments_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c51e3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[CLS]', b'Sp', b'[MASK]', b'bob', b'Sq', b'[SEP]', b'[MASK]',\n",
       "  b'##ack', b'Ob', b'##ama', b'[SEP]'],\n",
       " [b'[CLS]', b'Mar', b'[MASK]', b'A', b'##ven', b'[SEP]', b'President',\n",
       "  b'is', b'the', b'[MASK]', b'[SEP]']]>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(_VOCAB, masked_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6120c496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[2, 6],\n",
       " [2, 9]]>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a43904c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[9, 16],\n",
       " [12, 27]]>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_lm_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6247983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'##onge', b'Bar'],\n",
       " [b'##vel', b'highest']]>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(_VOCAB, masked_lm_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec278f",
   "metadata": {},
   "source": [
    "# Padding Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8eab8566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_word_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[ 3, 22,  1, 24, 23,  4,  1,  5],\n",
       "        [ 3, 18,  1, 15, 13,  4, 21, 28]])>,\n",
       " 'input_mask': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])>,\n",
       " 'input_type_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1]])>,\n",
       " 'masked_lm_ids': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[ 9, 16,  0,  0,  0],\n",
       "        [12, 27,  0,  0,  0]])>,\n",
       " 'masked_lm_positions': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[2, 6, 0, 0, 0],\n",
       "        [2, 9, 0, 0, 0]])>,\n",
       " 'masked_lm_weights': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[1, 1, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0]])>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare and pad combined segment inputs\n",
    "input_word_ids, input_mask = text.pad_model_inputs(\n",
    "  masked_token_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "input_type_ids, _ = text.pad_model_inputs(\n",
    "  segments_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "\n",
    "# Prepare and pad masking task inputs\n",
    "masked_lm_positions, masked_lm_weights = text.pad_model_inputs(\n",
    "  masked_pos, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "masked_lm_ids, _ = text.pad_model_inputs(\n",
    "  masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "\n",
    "model_inputs = {\n",
    "    \"input_word_ids\": input_word_ids,\n",
    "    \"input_mask\": input_mask,\n",
    "    \"input_type_ids\": input_type_ids,\n",
    "    \"masked_lm_ids\": masked_lm_ids,\n",
    "    \"masked_lm_positions\": masked_lm_positions,\n",
    "    \"masked_lm_weights\": masked_lm_weights,\n",
    "}\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb4b51d",
   "metadata": {},
   "source": [
    "# Brinigng all preprocessing steps together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18ee328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_pretrain_preprocess(vocab_table, features):\n",
    "  # Input is a string Tensor of documents, shape [batch, 1].\n",
    "  text_a = features[\"text_a\"]\n",
    "  text_b = features[\"text_b\"]\n",
    "\n",
    "  # Tokenize segments to shape [num_sentences, (num_words)] each.\n",
    "  tokenizer = text.BertTokenizer(\n",
    "      vocab_table,\n",
    "      token_out_type=tf.int64)\n",
    "  segments = [tokenizer.tokenize(text).merge_dims(\n",
    "      1, -1) for text in (text_a, text_b)]\n",
    "\n",
    "  # Truncate inputs to a maximum length.\n",
    "  trimmer = text.RoundRobinTrimmer(max_seq_length=6)\n",
    "  trimmed_segments = trimmer.trim(segments)\n",
    "\n",
    "  # Combine segments, get segment ids and add special tokens.\n",
    "  segments_combined, segment_ids = text.combine_segments(\n",
    "      trimmed_segments,\n",
    "      start_of_sequence_id=_START_TOKEN,\n",
    "      end_of_segment_id=_END_TOKEN)\n",
    "\n",
    "  # Apply dynamic masking task.\n",
    "  masked_input_ids, masked_lm_positions, masked_lm_ids = (\n",
    "      text.mask_language_model(\n",
    "        segments_combined,\n",
    "        random_selector,\n",
    "        mask_values_chooser,\n",
    "      )\n",
    "  )\n",
    "\n",
    "  # Prepare and pad combined segment inputs\n",
    "  input_word_ids, input_mask = text.pad_model_inputs(\n",
    "    masked_input_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "  input_type_ids, _ = text.pad_model_inputs(\n",
    "    segment_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "\n",
    "  # Prepare and pad masking task inputs\n",
    "  masked_lm_positions, masked_lm_weights = text.pad_model_inputs(\n",
    "    masked_lm_positions, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "  masked_lm_ids, _ = text.pad_model_inputs(\n",
    "    masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "\n",
    "  model_inputs = {\n",
    "      \"input_word_ids\": input_word_ids,\n",
    "      \"input_mask\": input_mask,\n",
    "      \"input_type_ids\": input_type_ids,\n",
    "      \"masked_lm_ids\": masked_lm_ids,\n",
    "      \"masked_lm_positions\": masked_lm_positions,\n",
    "      \"masked_lm_weights\": masked_lm_weights,\n",
    "  }\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1c66299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_word_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[ 3, 22,  1, 24,  4,  1,  5, 19],\n",
       "        [ 3, 18, 12, 15,  4, 21, 28,  1]])>,\n",
       " 'input_mask': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])>,\n",
       " 'input_type_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[0, 0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1]])>,\n",
       " 'masked_lm_ids': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[ 9, 16,  0,  0,  0],\n",
       "        [21, 30,  0,  0,  0]])>,\n",
       " 'masked_lm_positions': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[2, 5, 0, 0, 0],\n",
       "        [5, 7, 0, 0, 0]])>,\n",
       " 'masked_lm_weights': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[1, 1, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0]])>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = (\n",
    "    tf.data.Dataset.from_tensors(examples)\n",
    "    .map(functools.partial(bert_pretrain_preprocess, lookup_table))\n",
    ")\n",
    "\n",
    "next(iter(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject-ker",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
